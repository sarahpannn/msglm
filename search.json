[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "from IPython.display import Image, display\nfrom pathlib import Path\nfrom pathlib import Path",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#api-exploration",
    "href": "core.html#api-exploration",
    "title": "core",
    "section": "API Exploration",
    "text": "API Exploration\nAnthropic‚Äôs Claude and OpenAI‚Äôs GPT models are some of the most popular LLMs.\nLet‚Äôs take a look at their APIs and to learn how we should structure our messages for a simple text chat.\n\nopenai\n\nfrom openai import OpenAI\n\n\nclient = OpenAI()\n\nclient.responses.create(\n  model=\"gpt-4.1\",\n  input=[ {\"role\": \"user\", \"content\": \"Hello, world!\"} ]\n)\n\nHello, world! üëã How can I assist you today?\n\n\nid: resp_6861e1ad4b7c81a1b4d6c1cc66589bbe0972cbf25d9578d3\ncreated_at: 1751245229.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-4.1-2025-04-14\nobject: response\noutput: [ResponseOutputMessage(id=‚Äòmsg_6861e1ad9a1081a1a83cbc85bdf794ac0972cbf25d9578d3‚Äô, content=[ResponseOutputText(annotations=[], text=‚ÄòHello, world! üëã How can I assist you today?‚Äô, type=‚Äòoutput_text‚Äô, logprobs=[])], role=‚Äòassistant‚Äô, status=‚Äòcompleted‚Äô, type=‚Äòmessage‚Äô)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nmax_output_tokens: None\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nreasoning: Reasoning(effort=None, generate_summary=None, summary=None)\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‚Äòtext‚Äô))\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=11, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=25)\nuser: None\nstore: True\n\n\n\n\n\n\nanthropic\n\nfrom anthropic import Anthropic\n\n\nclient = Anthropic()\n\nclient.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    messages=[ {\"role\": \"user\", \"content\": \"Hello, world!\"} ]\n)\n\nHello! It‚Äôs nice to meet you. How can I assist you today?\n\n\nid: msg_01SB8Rrc2eJ3okM8A5zwpHTf\ncontent: [{'citations': None, 'text': \"Hello! It's nice to meet you. How can I assist you today?\", 'type': 'text'}]\nmodel: claude-3-haiku-20240307\nrole: assistant\nstop_reason: end_turn\nstop_sequence: None\ntype: message\nusage: {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 11, 'output_tokens': 19, 'server_tool_use': None, 'service_tier': 'standard'}\n\n\n\n\nAs we can see both APIs use the exact same message structure.\n\n\nmk_msg\nOk, let‚Äôs build the first version of mk_msg to handle this case\n\ndef mk_msg(content:str, role:str=\"user\")-&gt;dict:\n    \"Create an OpenAI/Anthropic compatible message.\"\n    return dict(role=role, content=content)\n\nLet‚Äôs test it out with the OpenAI API. To do that we‚Äôll need to setup two things:\n\ninstall the openai SDK by running pip install openai\nadd your openai api key to your env vars export OPENAI_API_KEY=\"YOUR_OPEN_API_KEY\"\n\n\noa_cli = OpenAI()\n\nr = oa_cli.responses.create(\n  model=\"gpt-4o-mini\",\n  input=[mk_msg(\"Hello, world!\")]\n)\nr.output_text\n\n'Hello! How can I assist you today?'\n\n\nNow, let‚Äôs test out mk_msg on the Anthropic API. To do that we‚Äôll need to setup two things:\n\ninstall the openai SDK by running pip install anthropic\nadd your anthropic api key to your env vars export ANTHROPIC_API_KEY=\"YOUR_ANTHROPIC_API_KEY\"\n\n\na_cli = Anthropic()\n\nr = a_cli.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    messages=[mk_msg(\"Hello, world!\")]\n)\nr.content[0].text\n\n\"Hello! It's great to meet you. How can I assist you today?\"\n\n\nSo far so good!\n\nHelper Functions\nBefore going any further, let‚Äôs create some helper functions to make it a little easier to call the OpenAI and Anthropic APIs. We‚Äôre going to be making a bunch of API calls to test our code and typing the full expressions out each time will become a little tedious. These functions won‚Äôt be included in the final package.\n\ndef openai_chat(msgs: list)-&gt;tuple:\n    \"call the openai chat responses endpoint with `msgs`.\"\n    r = oa_cli.responses.create(model=\"o4-mini\", input=msgs)\n    return r, r.output_text\n\nLet‚Äôs double check that mk_msg still works with our simple text example from before.\n\n_, text = openai_chat([mk_msg(\"Hello, world!\")])\ntext\n\n'Hello there! How can I assist you today?'\n\n\n\ndef anthropic_chat(msgs: list)-&gt;tuple:\n    \"call the anthropic messages endpoint with `msgs`.\"\n    r = a_cli.messages.create(model=\"claude-sonnet-4-20250514\", max_tokens=1024, messages=msgs)\n    return r, r.content[0].text\n\nand Anthropic‚Ä¶\n\n_, text = anthropic_chat([mk_msg(\"Hello, world!\")])\ntext\n\n'Hello! Nice to meet you! How are you doing today? Is there anything I can help you with?'\n\n\n\n\n\nImages\nOk, let‚Äôs see how both APIs handle image messages.\n\n\nopenai\n\nimport base64, httpx\n\n\nimg_url = \"https://claudette.answer.ai/index_files/figure-html/cell-35-output-1.jpeg\"\n\n\nmtype = \"image/jpeg\"\nimg_content = httpx.get(img_url).content\n\n\nimg = base64.b64encode(img_content).decode(\"utf-8\")\n\nclient = OpenAI()\nr = client.responses.create(\n    model=\"gpt-4o-mini\",\n    input=[\n        {\n            \"role\":\"user\",\n            \"content\": [\n                {\"type\":\"input_text\",\"text\":\"What's in this image?\"},\n                {\"type\":\"input_image\",\"image_url\":f\"data:image/jpeg;base64,{img}\"},\n            ],\n        }\n    ],\n)\nr.output_text\n\n'The image contains a puppy lying on the grass near some flowers. The puppy has a white coat with brown markings and appears to be playful and curious. The setting seems to be outdoors, with greenery and blooming flowers in the background.'\n\n\n\n\nanthropic\n\nmtype = \"image/jpeg\"\nimg = base64.b64encode(img_content).decode(\"utf-8\")\n\nclient = Anthropic()\nr = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\":\"user\",\n            \"content\": [\n                {\"type\":\"text\",\"text\":\"What's in this image?\"},\n                {\"type\":\"image\",\"source\":{\"type\":\"base64\",\"media_type\":mtype,\"data\":img}}\n            ],\n        }\n    ],\n)\nr.content[0].text\n\n\"This image shows a cute puppy lying in a grassy area with purple flowers in the background. The puppy appears to be a Cavalier King Charles Spaniel, with a long, silky coat in a reddish-brown color with white markings. The puppy has a friendly, inquisitive expression on its face as it gazes directly at the camera. The image conveys a sense of tranquility and natural beauty, with the vibrant purple flowers providing a lovely contrast to the puppy's warm coloring.\"\n\n\nBoth APIs format images slightly differently and the structure of the message content is a little more complex.\nIn a text chat, content is a simple string but for a multimodal chat (text+images) we can see that content is a list of dictionaries.",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#msg-class",
    "href": "core.html#msg-class",
    "title": "core",
    "section": "Msg Class",
    "text": "Msg Class\n\nBasics\nLet‚Äôs create _mk_img to make our code a little DRY‚Äôr.\n\n\nExported source\ndef _mk_img(data:bytes)-&gt;tuple:\n    \"Convert image bytes to a base64 encoded image\"\n    img = base64.b64encode(data).decode(\"utf-8\")\n    mtype = mimetypes.types_map[\".\"+imghdr.what(None, h=data)]\n    return img, mtype\n\n\nTo handle the additional complexity of multimodal messages let‚Äôs build a Msg class for the content data structure:\n{\n    \"role\": \"user\",\n    \"content\": [{\"type\": \"text\", \"text\": \"What's in this image?\"}],\n}\n\nsource\n\n\nMsg\n\n Msg ()\n\nHelper class to create a message for the OpenAI and Anthropic APIs.\n\n\nExported source\nclass Msg:\n    \"Helper class to create a message for the OpenAI and Anthropic APIs.\"\n    pass\n\n\nAs both APIs handle images differently let‚Äôs subclass Msg for each API and handle the image formatting in a method called img_msg.\n\nsource\n\n\nOpenAiMsg\n\n OpenAiMsg ()\n\nHelper class to create a message for the OpenAI API.\n\n\nExported source\nclass OpenAiMsg(Msg):\n    \"Helper class to create a message for the OpenAI API.\"\n    pass\n\n\n\nsource\n\n\nAnthropicMsg\n\n AnthropicMsg ()\n\nHelper class to create a message for the Anthropic API.\n\n\nExported source\nclass AnthropicMsg(Msg):\n    \"Helper class to create a message for the Anthropic API.\"\n    pass\n\n\nLet‚Äôs write some helper functions for mk_content to use.\n\n\nExported source\ndef _is_img(data): return isinstance(data, bytes) and bool(imghdr.what(None, data))\n\n\nA PDF file should start with %PDF followed by the pdf version %PDF-1.1\n\n\nExported source\ndef _is_pdf(data): \n    is_byte_pdf = isinstance(data, bytes) and data.startswith(b'%PDF-')\n    is_pdf_url = isinstance(data, str) and (data.startswith(\"http\") and data.endswith(\".pdf\") or\n   'pdf' in data.split('/'))\n    return is_byte_pdf or is_pdf_url\n\n\n\nprint(_is_pdf(\"https://arxiv.org/pdf/2301.00001\"))\nprint(_is_pdf(\"https://arxiv.org/abs/2301.00001\"))\nprint(_is_pdf(\"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"))\n\nTrue\nFalse\nTrue\n\n\nWe create an appropriate type based on content:\n\nsource\n\n\nMsg.mk_content\n\n Msg.mk_content (content, text_only=False)\n\n\n\nExported source\n@patch\ndef mk_content(self:Msg, content, text_only=False)-&gt;dict:\n    if _is_img(content): \n        return self.img_msg(content)\n    if _is_pdf(content): \n        return self.pdf_msg(content)\n    if isinstance(content, str): return self.text_msg(content, text_only=text_only)\n    return content\n\n\n‚Ä¶then we call the model with this content:\n\n@patch\ndef __call__(self:Msg, role:str, content:[list, str], text_only:bool=False, **kw)-&gt;dict:\n    \"Create an OpenAI/Anthropic compatible message with `role` and `content`.\"\n    if content is not None and not isinstance(content, list): content = [content]\n    content = [self.mk_content(o, text_only=text_only) for o in content] if content else ''\n    return dict(role=role, content=content[0] if text_only else content, **kw)\n\nOpenAI implementations:\n\nsource\n\n\nOpenAiMsg.text_msg\n\n OpenAiMsg.text_msg (s:str, text_only=False)\n\nConvert s to a text message\n\n\nExported source\n@patch\ndef img_msg(self:OpenAiMsg, data:bytes)-&gt;dict:\n    \"Convert `data` to an image message\"\n    img, mtype = _mk_img(data)\n    return {\"type\": \"input_image\", \"image_url\": f\"data:{mtype};base64,{img}\"}\n\n@patch\ndef text_msg(self:OpenAiMsg, s:str, text_only=False)-&gt;dict: \n    \"Convert `s` to a text message\"\n    return s if text_only else {\"type\": \"input_text\", \"text\":s}\n\n\n\nsource\n\n\nOpenAiMsg.img_msg\n\n OpenAiMsg.img_msg (data:bytes)\n\nConvert data to an image message\nAnthropic implementations:\n\nsource\n\n\nAnthropicMsg.text_msg\n\n AnthropicMsg.text_msg (s:str, text_only=False)\n\nConvert s to a text message\n\n\nExported source\n@patch\ndef img_msg(self:AnthropicMsg, data:bytes)-&gt;dict:\n    \"Convert `data` to an image message\"\n    img, mtype = _mk_img(data)\n    r = {\"type\": \"base64\", \"media_type\": mtype, \"data\":img}\n    return {\"type\": \"image\", \"source\": r}\n\n@patch\ndef text_msg(self:AnthropicMsg, s:str, text_only=False)-&gt;dict: \n    \"Convert `s` to a text message\"\n    return s if text_only else {\"type\": \"text\", \"text\":s}\n\n\n\nsource\n\n\nAnthropicMsg.img_msg\n\n AnthropicMsg.img_msg (data:bytes)\n\nConvert data to an image message\nUpdate mk_msg to use Msg.\n\nsource\n\n\nmk_msg\n\n mk_msg (content:Union[list,str], role:str='user', *args,\n         api:str='openai', **kw)\n\nCreate an OpenAI/Anthropic compatible message.\n\nmk_msg([\"Hello world\", \"how are you?\"], api='openai')\n\n{ 'content': [ {'text': 'Hello world', 'type': 'input_text'},\n               {'text': 'how are you?', 'type': 'input_text'}],\n  'role': 'user'}\n\n\n\nmk_msg([\"Hello world\", \"how are you?\"], api='anthropic')\n\n{ 'content': [ {'text': 'Hello world', 'type': 'text'},\n               {'text': 'how are you?', 'type': 'text'}],\n  'role': 'user'}\n\n\n\nmsg = mk_msg([img_content, \"describe this picture\"], api=\"openai\")\n_, text = openai_chat([msg])\ntext\n\n'A small puppy, likely a young spaniel, is lying in green grass beside a pot of purple daisy-like flowers. Key details:  \\n‚Ä¢ Coat: Soft white fur with rich chestnut-brown patches, especially around its floppy ears and eyes.  \\n‚Ä¢ Pose: Front paws stretched forward, body low to the ground, head slightly tilted, looking straight at the camera with a curious, gentle expression.  \\n‚Ä¢ Setting: Bright daylight, fresh green lawn, and clusters of delicate purple blooms tucked into a terracotta or wooden planter just behind the puppy.  \\n‚Ä¢ Mood: Calm and inquisitive‚Äîits wide eyes and relaxed posture give the impression it‚Äôs quietly exploring its surroundings.'\n\n\n\nmsg = mk_msg([img_content, \"describe this picture\"], api=\"anthropic\")\n_, text = anthropic_chat([msg])\ntext\n\n\"This is an adorable photograph of a young puppy, likely a Cavalier King Charles Spaniel or similar breed, with beautiful reddish-brown and white fur markings. The puppy has distinctive coloring with a white face featuring a brown patch around one eye, and longer, silky ears that are a rich auburn color. \\n\\nThe puppy is positioned on green grass and appears to be resting or lying down near some purple flowers, which look like small daisies or asters. The setting appears to be outdoors in a garden area, with what looks like a brick or stone structure in the background. The lighting gives the photo a warm, natural feel, and the puppy's expression is sweet and gentle, looking directly at the camera with dark, soulful eyes. The overall composition creates a charming, pastoral scene that highlights the puppy's natural beauty.\"\n\n\n\n\nPDFs\nWhat about chatting with PDFs? Unfortunately, OpenAI‚Äôs message completions API doesn‚Äôt offer PDF support at the moment, but Claude does.\nUnder the hood, Claude extracts the text from the PDF and converts each page to an image. This means you can ask Claude about any text, pictures, charts, and tables in the PDF. Here‚Äôs an example from the Claude docs. Overall the message structure is pretty similar to an image message.\npdf_url = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\npdf_data = base64.standard_b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\nclient = anthropic.Anthropic()\nmessage = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\", max_tokens=1024,\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"document\",\n                \"source\": { \"type\": \"base64\", \"media_type\": \"application/pdf\", \"data\": pdf_data }\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"Which model has the highest human preference win rates across each use-case?\"\n            }\n        ]\n    }]\n)\nThe Anthropic API has since offered an option for PDFs that can be accessed online via url.\nclient = anthropic.Anthropic()\nmessage = client.messages.create(\n    model=\"claude-opus-4-20250514\",\n    max_tokens=1024,\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"url\",\n                        \"url\": \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\n                    }\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What are the key findings in this document?\"\n                }\n            ]\n        }\n    ],\n)\nLet‚Äôs create a method that converts a byte string to the base64 encoded string that Anthropic expects.\n\n\nExported source\ndef _mk_pdf(data:bytes)-&gt;str:\n    \"Convert pdf bytes to a base64 encoded pdf\"\n    return base64.standard_b64encode(data).decode(\"utf-8\")\n\n\nWe add a pdf_msg method to AnthropicMsg that uses _mk_pdf.\n\nsource\n\n\nAnthropicMsg.pdf_msg\n\n AnthropicMsg.pdf_msg (data:bytes|str)\n\nConvert data to a pdf message\n\n\nExported source\n@patch\ndef pdf_msg(self:AnthropicMsg, data: bytes | str) -&gt; dict:\n    \"Convert `data` to a pdf message\"\n    if isinstance(data, bytes):\n        r = {\"type\": \"base64\", \"media_type\": \"application/pdf\", \"data\":_mk_pdf(data)}\n    elif isinstance(data, str):\n        r = {\"type\": \"url\", \"url\": data}\n    return {\"type\": \"document\", \"source\": r}\n\n\nLet‚Äôs test our changes on a financial report.\n\npdf = Path('financial_report.pdf').read_bytes()\nmsg = mk_msg([pdf, \"what was the average monthly revenue for product D?\"], api=\"anthropic\")\n_, text = anthropic_chat([msg])\ntext\n\n'Looking at the Product D chart on page 5, I can read the monthly revenue values from the bar chart:\\n\\n- January: ~900\\n- February: ~500\\n- March: ~400\\n- April: ~700\\n- May: ~800\\n- June: ~900\\n- July: ~1000\\n- August: ~1050\\n- September: ~1200\\n- October: ~1300\\n- November: ~1300\\n- December: ~1300\\n\\nAdding these values: 900 + 500 + 400 + 700 + 800 + 900 + 1000 + 1050 + 1200 + 1300 + 1300 + 1300 = 11,350\\n\\nThe average monthly revenue for Product D was 11,350 √∑ 12 = **$946** (rounded to the nearest dollar).'\n\n\n\npdf_url = \"https://arxiv.org/pdf/2506.18880\"\nmsg = mk_msg([pdf_url, \"What were the three types of generalization the authors of this paper looked at?\"], api=\"anthropic\")\n_, text = anthropic_chat([msg])\ntext\n\n\"According to the paper, the three types of generalization the authors examined were:\\n\\n1. **Exploratory Generalization** - Applying known problem-solving skills to more complex instances within the same problem domain. For example, counting rectangles in an octagon (training) versus a dodecagon (test).\\n\\n2. **Compositional Generalization** - Combining distinct reasoning skills that were previously learned in isolation to solve novel problems that require integrating these skills in new and coherent ways. For example, combining GCD computation with polynomial root-finding.\\n\\n3. **Transformative Generalization** - Adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. For example, replacing brute-force enumeration with a subtractive counting method that overcounts and then removes invalid cases.\\n\\nThese three axes of generalization were inspired by Margaret Boden's typology of creativity in cognitive science. The authors designed OMEGA to systematically evaluate large language models across these dimensions using controlled training-test pairs that isolate specific reasoning capabilities within mathematical problem-solving contexts.\"\n\n\n\n\nConversation\nLLMs are stateless. To continue a conversation we need to include the entire message history in every API call. By default the role in each message alternates between user and assistant.\nLet‚Äôs add a method that alternates the roles for us and then calls mk_msgs.\n\ndef mk_msgs(msgs: list, *args, api:str=\"openai\", **kw) -&gt; list:\n    \"Create a list of messages compatible with OpenAI/Anthropic.\"\n    if isinstance(msgs, str): msgs = [msgs]\n    return [mk_msg(o, ('user', 'assistant')[i % 2], *args, api=api, **kw) for i, o in enumerate(msgs)]\n\n\nmk_msgs([\"Hello\", \"Some assistant response\", \"tell me a joke\"])\n\n[{'role': 'user', 'content': 'Hello'},\n {'role': 'assistant', 'content': 'Some assistant response'},\n {'role': 'user', 'content': 'tell me a joke'}]\n\n\n\n\nSDK Objects\nTo make our lives even easier, it would be nice if mk_msg could format the SDK objects returned from a previous chat so that we can pass them straight to mk_msgs.\nThe OpenAI SDK accepts objects like ChatCompletion as messages. Anthropic is different and expects every message to have the role, content format that we‚Äôve seen so far.\n\nsource\n\n\nMsg.__call__\n\n Msg.__call__ (role:str, content:[&lt;class'list'&gt;,&lt;class'str'&gt;],\n               text_only:bool=False, **kw)\n\nCreate an OpenAI/Anthropic compatible message with role and content.\n\nsource\n\n\nAnthropicMsg.find_block\n\n AnthropicMsg.find_block (r)\n\nFind the message in r.\n\nsource\n\n\nAnthropicMsg.is_sdk_obj\n\n AnthropicMsg.is_sdk_obj (r)\n\nCheck if r is an SDK object.\n\nsource\n\n\nOpenAiMsg.find_block\n\n OpenAiMsg.find_block (r)\n\nFind the message in r.\n\nsource\n\n\nOpenAiMsg.is_sdk_obj\n\n OpenAiMsg.is_sdk_obj (r)\n\nCheck if r is an SDK object.\n\nsource\n\n\nmk_msgs\n\n mk_msgs (msgs:list, *args, api:str='openai', **kw)\n\nCreate a list of messages compatible with OpenAI/Anthropic.\nLet‚Äôs test our changes.\n\nmsgs = [\"tell me a joke\"]\nr, text = openai_chat(mk_msgs(msgs))\ntext\n\n'Why don‚Äôt scientists trust atoms?  \\nBecause they make up everything!'\n\n\n\nmsgs += [r, \"tell me another joke that's similar to your first joke\"]\nmm = mk_msgs(msgs)\nmm\n\n[{'role': 'user', 'content': 'tell me a joke'},\n ResponseReasoningItem(id='rs_6861ee59eb80819d9b284f7e3592abd50325e602e0c97467', summary=[], type='reasoning', encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_6861ee5ad01c819d86aa11520c3c9e0b0325e602e0c97467', content=[ResponseOutputText(annotations=[], text='Why don‚Äôt scientists trust atoms?  \\nBecause they make up everything!', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n {'role': 'user',\n  'content': \"tell me another joke that's similar to your first joke\"}]\n\n\n\nr, text = openai_chat(mm)\ntext\n\n'Why don‚Äôt scientists trust electrons?  \\nBecause they‚Äôre always negative!'\n\n\n\n\nUsage\nTo make msglm a little easier to use let‚Äôs create OpenAI and Anthropic wrappers for mk_msg and mk_msgs.\n\nmk_msg_anthropic = partial(mk_msg, api=\"anthropic\")\nmk_msgs_anthropic = partial(mk_msgs, api=\"anthropic\")\n\nIf you‚Äôre using OpenAI you should be able to use the import below\nfrom msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\nSimilarily for Anthropic\nfrom msglm import mk_msg_anthropic as mk_msg, mk_msgs_anthropic as mk_msgs",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "core.html#extra-features",
    "href": "core.html#extra-features",
    "title": "core",
    "section": "Extra features",
    "text": "Extra features\n\nCaching\nAnthropic currently offers prompt caching, which can reduce cost and latency.\nTo cache a message, we simply add a cache_control field to our content as shown below.\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Hello, can you tell me more about the solar system?\",\n            \"cache_control\": {\"type\": \"ephemeral\"}\n        }\n    ]\n}\nLet‚Äôs update our mk_msg and mk_msgs Anthropic wrappers to support caching.\n\nsource\n\n\nmk_msgs_anthropic\n\n mk_msgs_anthropic (*args, cache=False, cache_last_ckpt_only=False,\n                    api:str='openai')\n\nCreate a list of Anthropic compatible messages.\n\nsource\n\n\nmk_msg_anthropic\n\n mk_msg_anthropic (*args, cache=False, role:str='user', api:str='openai')\n\nCreate an Anthropic compatible message.\nLet‚Äôs see caching in action\n\nmk_msg_anthropic(\"Don't cache my message\")\n\n{'content': \"Don't cache my message\", 'role': 'user'}\n\n\n\nmk_msg_anthropic(\"Please cache my message\", cache=True)\n\n{ 'content': [ { 'cache_control': {'type': 'ephemeral'},\n                 'text': 'Please cache my message',\n                 'type': 'text'}],\n  'role': 'user'}\n\n\n\n\nCitations\nThe Anthropic API provides detailed citations when answering questions about documents.\nWhen citations are enabled a citations block like the one below will be included in the response.\n{\n  \"content\": [\n    { \"type\": \"text\", \"text\": \"According to the document, \" },\n    {\n      \"type\": \"text\", \"text\": \"the grass is green\",\n      \"citations\": [{\n        \"type\": \"char_location\",\n        \"cited_text\": \"The grass is green.\",\n        \"document_index\": 0, \"document_title\": \"Example Document\",\n        \"start_char_index\": 0, \"end_char_index\": 20\n      }]\n    }\n  ]\n}\nTo enable citations you need to create an Anthropic document with the following structure.\n{\n    \"type\": \"document\",\n    \"source\": {...},\n    \"title\": \"Document Title\", # optional\n    \"context\": \"Context about the document that will not be cited from\", # optional\n    \"citations\": {\"enabled\": True}\n}\nCurrently Anthropic supports citations on 3 document types: - text - pdfs - custom\nA text document has the following source structure.\n{\"type\": \"text\", \"media_type\": \"text/plain\", \"data\": \"Plain text content...\"}\nHere‚Äôs the source structure for a pdf.\n{\"type\": \"base64\", \"media_type\": \"application/pdf\", \"data\": b64_enc_data}\nFinally, here‚Äôs the source structure for a custom document.\n{\n  \"type\": \"content\",\n  \"content\": [\n    {\"type\": \"text\", \"text\": \"First chunk\"},\n    {\"type\": \"text\", \"text\": \"Second chunk\"}\n  ]\n}\n\nsource\n\n\nmk_ant_doc\n\n mk_ant_doc (content, title=None, context=None, citation=True, **kws)\n\nCreate an Anthropic document.\nHere‚Äôs how you would implement the example from the citation‚Äôs docs.\n\ndoc = mk_ant_doc(\"The grass is green. The sky is blue.\", title=\"My Document\", context=\"This is a trustworthy document.\")\nmk_msg([doc, \"What color is the grass and sky?\"])\n\n{ 'content': [ { 'citations': {'enabled': True},\n                 'context': 'This is a trustworthy document.',\n                 'source': { 'data': 'The grass is green. The sky is blue.',\n                             'media_type': 'text/plain',\n                             'type': 'text'},\n                 'title': 'My Document',\n                 'type': 'document'},\n               { 'text': 'What color is the grass and sky?',\n                 'type': 'input_text'}],\n  'role': 'user'}",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "msglm",
    "section": "",
    "text": "Install the latest version from pypi\n$ pip install msglm",
    "crumbs": [
      "msglm"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "msglm",
    "section": "Usage",
    "text": "Usage\nTo use an LLM we need to structure our messages in a particular format.\nHere‚Äôs an example of a text chat from the OpenAI docs.\nfrom openai import OpenAI\nclient = OpenAI()\n\ncompletion = client.chat.completions.create(\n  model=\"gpt-4o\",\n  messages=[\n    {\"role\": \"user\", \"content\": \"What's the Wild Atlantic Way?\"}\n  ]\n)\nGenerating the correct format for a particular API can get tedious. The goal of msglm is to make it easier.\nThe examples below will show you how to use msglm for text and image chats with OpenAI and Anthropic.\n\nText Chats\nFor a text chat simply pass a list of strings and the api format (e.g.¬†‚Äúopenai‚Äù) to mk_msgs and it will generate the correct format.\nmk_msgs([\"Hello, world!\", \"some assistant response\"], api=\"openai\")\n[\n    {\"role\": \"user\", \"content\": \"Hello, world!\"},\n    {\"role\": \"assistant\", \"content\": \"Some assistant response\"}\n]\n\nanthropic\nfrom msglm import mk_msgs_anthropic as mk_msgs\nfrom anthropic import Anthropic\nclient = Anthropic()\n\nr = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    messages=[mk_msgs([\"Hello, world!\", \"some LLM response\"])]\n)\nprint(r.content[0].text)\n\n\nopenai\nfrom msglm import mk_msgs_openai as mk_msgs\nfrom openai import OpenAI\n\nclient = OpenAI()\nr = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[mk_msgs([\"Hello, world!\", \"some LLM response\"])]\n)\nprint(r.choices[0].message.content)\n\n\n\nImage Chats\nFor an image chat simply pass the raw image bytes in a list with your question to mk_msgs and it will generate the correct format.\nmk_msg([img, \"What's in this image?\"], api=\"anthropic\")\n[\n    {\n        \"role\": \"user\", \n        \"content\": [\n            {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": media_type, \"data\": img}}\n            {\"type\": \"text\", \"text\": \"What's in this image?\"}\n        ]\n    }\n]\n\nanthropic\nimport httpx\nfrom msglm import mk_msg_anthropic as mk_msg\nfrom anthropic import Anthropic\n\nclient = Anthropic()\n\nimg_url = \"https://www.atshq.org/wp-content/uploads/2022/07/shutterstock_1626122512.jpg\"\nimg = httpx.get(img_url).content\n\nr = client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    max_tokens=1024,\n    messages=[mk_msg([img, \"Describe the image\"])]\n)\nprint(r.content[0].text)\n\n\nopenai\nimport httpx\nfrom msglm import mk_msg_openai as mk_msg\nfrom openai import OpenAI\n\nimg_url = \"https://www.atshq.org/wp-content/uploads/2022/07/shutterstock_1626122512.jpg\"\nimg = httpx.get(img_url).content\n\nclient = OpenAI()\nr = client.chat.completions.create(\n  model=\"gpt-4o-mini\",\n  messages=[mk_msg([img, \"Describe the image\"])]\n)\nprint(r.choices[0].message.content)\n\n\n\nAPI Wrappers\nTo make life a little easier, msglm comes with api specific wrappers for mk_msg and mk_msgs.\nFor Anthropic use\nfrom msglm import mk_msg_anthropic as mk_msg, mk_msgs_anthropic as mk_msgs\nFor OpenAI use\nfrom msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n\n\nOther use-cases\n\nPrompt Caching\nmsglm supports prompt caching for Anthropic models. Simply pass cache=True to mk_msg or mk_msgs.\nfrom msglm import mk_msg_anthropic as mk_msg\n\nmk_msg(\"please cache my message\", cache=True)\nThis generates the expected cache block below\n{\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"Please cache my message\", \"cache_control\": {\"type\": \"ephemeral\"}}\n    ]\n}\n\n\nPDF chats\nmsglm offers PDF support for Anthropic. Just like an image chat all you need to do is pass the raw pdf bytes in a list with your question to mk_msg and it will generate the correct format as shown in the example below.\nimport httpx\nfrom msglm import mk_msg_anthropic as mk_msg\nfrom anthropic import Anthropic\n\nclient = Anthropic(default_headers={'anthropic-beta': 'pdfs-2024-09-25'})\n\nurl = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\npdf = httpx.get(url).content\n\nr = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    messages=[mk_msg([pdf, \"Which model has the highest human preference win rates across each use-case?\"])]\n)\nprint(r.content[0].text)\nNote: this feature is currently in beta so you‚Äôll need to:\n\nuse the Anthropic beta client (e.g.¬†anthropic.Anthropic(default_headers={'anthropic-beta': 'pdfs-2024-09-25'}))\nuse the claude-3-5-sonnet-20241022 model\n\n\n\nCitations\nmsglm supports Anthropic citations. All you need to do is pass the content of your document to mk_ant_doc and then pass the output to mk_msg along with your question as shown in the example below.\nfrom msglm import mk_ant_doc, mk_msg_anthropic as mk_msg\nfrom anthropic import Anthropic\n\nclient = Anthropic()\n\ndoc = mk_ant_doc(\"The grass is green. The sky is blue.\", title=\"My Document\")\n\nr = client.messages.create(\n    model=\"claude-3-5-sonnet-20241022\",\n    max_tokens=1024,\n    messages=[mk_msg([doc, \"What color is the grass and sky?\"])]\n)\nfor o in r.content:\n    if c:=getattr(o, 'citations', None): print(f\"{o.text}. source: {c[0]['cited_text']} from  {c[0]['document_title']}\")\n    else: print(o.text)\nNote: The citations feature is currently available on Claude 3.5 Sonnet (new) and 3.5 Haiku.\n\n\n\nSummary\nWe hope msglm will make your life a little easier when chatting to LLMs. To learn more about the package please read this doc.",
    "crumbs": [
      "msglm"
    ]
  }
]